# ğŸ“˜ LINEAR REGRESSION

### 1. Äá»‹nh nghÄ©a & Báº£n cháº¥t

**Linear Regression** lÃ  thuáº­t toÃ¡n há»c mÃ¡y thuá»™c nhÃ³m **Há»c cÃ³ giÃ¡m sÃ¡t (Supervised Learning)**

- **Má»¥c tiÃªu:** TÃ¬m ra má»™t má»‘i quan há»‡ tuyáº¿n tÃ­nh (Ä‘Æ°á»ng tháº³ng hoáº·c máº·t pháº³ng) mÃ´ táº£ tá»‘t nháº¥t sá»± phá»¥ thuá»™c giá»¯a biáº¿n Ä‘áº§u vÃ o ($X$) vÃ  biáº¿n káº¿t quáº£ ($y$)

- **Báº£n cháº¥t:** TÃ¬m má»™t Ä‘Æ°á»ng tháº³ng sao cho khoáº£ng cÃ¡ch tá»« cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u thá»±c táº¿ Ä‘áº¿n Ä‘Æ°á»ng tháº³ng Ä‘Ã³ lÃ  nhá» nháº¥t

[Image of linear regression best fit line]

### 2. CÃ¡c ThÃ nh pháº§n & CÃ´ng thá»©c ToÃ¡n há»c

Äá»ƒ xÃ¢y dá»±ng mÃ´ hÃ¬nh, ta cáº§n 3 thÃ nh pháº§n chÃ­nh: **HÃ m giáº£ thuyáº¿t**, **HÃ m máº¥t mÃ¡t** vÃ  **Thuáº­t toÃ¡n tá»‘i Æ°u**

#### A. HÃ m Giáº£ Thuyáº¿t (Hypothesis Function)

$$\hat{y} = wx + b$$

- **$\hat{y}$ (y-hat):** GiÃ¡ trá»‹ mÃ¡y dá»± Ä‘oÃ¡n
- **$x$ (Input):** Dá»¯ liá»‡u Ä‘áº§u vÃ o (Feature)
- **$w$ (Weight - Trá»ng sá»‘):** Äá»™ dá»‘c cá»§a Ä‘Æ°á»ng tháº³ng. Quyáº¿t Ä‘á»‹nh má»©c Ä‘á»™ áº£nh hÆ°á»Ÿng cá»§a $x$ lÃªn $y$
- **$b$ (Bias - Há»‡ sá»‘ tá»± do):** Äiá»ƒm cáº¯t trá»¥c tung. GiÃºp Ä‘Æ°á»ng tháº³ng tá»‹nh tiáº¿n lÃªn xuá»‘ng mÃ  khÃ´ng phá»¥ thuá»™c $x$

_(Náº¿u cÃ³ nhiá»u biáº¿n Ä‘áº§u vÃ o $x_1, x_2...$, cÃ´ng thá»©c lÃ : $\hat{y} = w_1x_1 + w_2x_2 + ... + b$)_

#### B. HÃ m Máº¥t MÃ¡t (Loss Function - MSE)

DÃ¹ng **Mean Squared Error (MSE)** Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh Ä‘ang thá»±c hiá»‡n tá»‘t hay dá»Ÿ

$$J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2$$

- **$m$:** Tá»•ng sá»‘ máº«u dá»¯ liá»‡u
- **$y^{(i)}$:** GiÃ¡ trá»‹ thá»±c táº¿ (nhÃ£n Ä‘Ãºng) cá»§a máº«u thá»© $i$
- **$\hat{y}^{(i)}$:** GiÃ¡ trá»‹ mÃ¡y vá»«a Ä‘oÃ¡n cho máº«u thá»© $i$
- **BÃ¬nh phÆ°Æ¡ng $(\dots)^2$:** GiÃºp triá»‡t tiÃªu dáº¥u Ã¢m vÃ  "trá»«ng pháº¡t" náº·ng cÃ¡c sai sá»‘ lá»›n (Outliers)
- **$\frac{1}{2m}$:** Chia trung bÃ¬nh. Sá»‘ $2$ á»Ÿ máº«u sá»‘ giÃºp triá»‡t tiÃªu sá»‘ mÅ© $2$ khi tÃ­nh Ä‘áº¡o hÃ m

#### C. Thuáº­t toÃ¡n Tá»‘i Æ°u (Gradient Descent)

DÃ¹ng Ä‘áº¡o hÃ m Ä‘á»ƒ biáº¿t hÆ°á»›ng "xuá»‘ng dá»‘c" nháº±m giáº£m thiá»ƒu sai sá»‘ $J$

**Quy táº¯c cáº­p nháº­t (VÃ²ng láº·p):**

$$w_{new} = w_{old} - \alpha \cdot \frac{\partial J}{\partial w}$$
$$b_{new} = b_{old} - \alpha \cdot \frac{\partial J}{\partial b}$$

- **$\alpha$ (Learning Rate):** Tá»‘c Ä‘á»™ há»c
  - Lá»›n quÃ¡: BÆ°á»›c nháº£y dÃ i, dá»… trÆ°á»£t qua Ä‘Ã¡y
  - Nhá» quÃ¡: Há»c ráº¥t cháº­m
- **$\frac{\partial J}{\partial w}, \frac{\partial J}{\partial b}$ (Gradient):** Äáº¡o hÃ m riÃªng, cho biáº¿t hÆ°á»›ng dá»‘c

**CÃ´ng thá»©c Gradient cá»¥ thá»ƒ (khi Ä‘Ã£ Ä‘áº¡o hÃ m xong):**

$$dw = \frac{1}{m} \sum (\hat{y} - y) \cdot x$$
$$db = \frac{1}{m} \sum (\hat{y} - y)$$

### 3. VÃ­ dá»¥ TÃ­nh toÃ¡n

Giáº£ sá»­ dá»¯ liá»‡u cÃ³ 1 máº«u duy nháº¥t: **Input $x=2$, Output thá»±c táº¿ $y=10$**

- Khá»Ÿi táº¡o: $w=3, b=1$. Learning rate $\alpha = 0.1$

**BÆ°á»›c 1: Dá»± Ä‘oÃ¡n (Forward Pass)**
$$\hat{y} = w \cdot x + b = 3 \cdot 2 + 1 = 7$$

**BÆ°á»›c 2: TÃ­nh sai sá»‘ (Loss)**
- Sai sá»‘ $e = \hat{y} - y = 7 - 10 = -3$
$$MSE = \frac{1}{2} (-3)^2 = 4.5$$

**BÆ°á»›c 3: TÃ­nh Gradient (Äáº¡o hÃ m)**

- $dw = (\hat{y} - y) \cdot x = (-3) \cdot 2 = -6$
- $db = (\hat{y} - y) = -3$

**BÆ°á»›c 4: Cáº­p nháº­t tham sá»‘ (Backward Pass)**

- $w_{má»›i} = 3 - 0.1 \cdot (-6) = 3 + 0.6 = 3.6$
- $b_{má»›i} = 1 - 0.1 \cdot (-3) = 1 + 0.3 = 1.3$

$\rightarrow$ **Káº¿t quáº£:** Sau 1 bÆ°á»›c há»c, $w$ tÄƒng tá»« 3 lÃªn 3.6, $b$ tÄƒng tá»« 1 lÃªn 1.3. Dá»± Ä‘oÃ¡n láº§n sau sáº½ lÃ  $3.6(2) + 1.3 = 8.5$ (Gáº§n vá»›i 10 hÆ¡n so vá»›i sá»‘ 7 ban Ä‘áº§u)

### 4. XÃ¢y dá»±ng mÃ´ hÃ¬nh

[Image of machine learning workflow steps]

1.  **Thu tháº­p & Táº£i dá»¯ liá»‡u:**

    - Äá»c file (CSV, Excel...)
    - XÃ¡c Ä‘á»‹nh Ä‘Ã¢u lÃ  Feature (X), Ä‘Ã¢u lÃ  Target (y)

2.  **Tiá»n xá»­ lÃ½ dá»¯ liá»‡u (Preprocessing):**

    - **LÃ m sáº¡ch:** Xá»­ lÃ½ dá»¯ liá»‡u thiáº¿u (NaN), dá»¯ liá»‡u rÃ¡c
    - **Chuáº©n hÃ³a (Normalization/Standardization):** ÄÆ°a dá»¯ liá»‡u vá» cÃ¹ng má»™t khoáº£ng (thÆ°á»ng dÃ¹ng Mean/Std). Náº¿u khÃ´ng lÃ m bÆ°á»›c nÃ y, thuáº­t toÃ¡n Gradient Descent sáº½ ráº¥t khÃ³ há»™i tá»¥
    - **Chia táº­p dá»¯ liá»‡u:** Train set vÃ  Test set

3.  **Thiáº¿t káº¿ MÃ´ hÃ¬nh:**

    - Chá»n thuáº­t toÃ¡n: Linear Regression (`nn.Linear`)
    - XÃ¡c Ä‘á»‹nh Input size (sá»‘ lÆ°á»£ng feature) vÃ  Output size (thÆ°á»ng lÃ  1)

4.  **Thiáº¿t láº­p huáº¥n luyá»‡n:**

    - Chá»n Loss Function: `MSELoss`
    - Chá»n Optimizer: `SGD` hoáº·c `Adam`
    - Chá»n Hyperparameters: Learning rate, Epochs

5.  **VÃ²ng láº·p huáº¥n luyá»‡n (Training Loop):**

    - Dá»± Ä‘oÃ¡n (Forward) $\rightarrow$ TÃ­nh Loss $\rightarrow$ Äáº¡o hÃ m (Backward) $\rightarrow$ Cáº­p nháº­t (Optimizer Step)

6.  **ÄÃ¡nh giÃ¡ & Dá»± Ä‘oÃ¡n:**
    - DÃ¹ng táº­p Test Ä‘á»ƒ kiá»ƒm tra Ä‘á»™ chÃ­nh xÃ¡c
    - Khi dá»± Ä‘oÃ¡n thá»±c táº¿: **Pháº£i chuáº©n hÃ³a input má»›i** theo quy táº¯c cá»§a táº­p train, sau Ä‘Ã³ **giáº£i chuáº©n hÃ³a output** Ä‘á»ƒ ra káº¿t quáº£ cuá»‘i cÃ¹ng

### 5. CÃ¡c lá»—i thÆ°á»ng gáº·p & LÆ°u Ã½

1.  **Underfitting (ChÆ°a há»c Ä‘Æ°á»£c gÃ¬):**

    - _Biá»ƒu hiá»‡n:_ ÄÆ°á»ng dá»± Ä‘oÃ¡n náº±m ngang, Loss khÃ´ng giáº£m
    - _LÃ½ do:_ Ãt dá»¯ liá»‡u, chá»n sai feature (feature rÃ¡c), hoáº·c model quÃ¡ Ä‘Æ¡n giáº£n
    - _Kháº¯c phá»¥c:_ ThÃªm dá»¯ liá»‡u, chá»n feature tá»‘t hÆ¡n (nhÆ° vÃ­ dá»¥ Ä‘á»•i tá»« `bedrooms` sang `income`), tÄƒng Epochs, tÄƒng Learning rate

2.  **QuÃªn chuáº©n hÃ³a (Normalization):**

    - Dáº«n Ä‘áº¿n viá»‡c $w$ vÃ  $b$ bá»‹ lá»‡ch láº¡c, Loss nháº£y lung tung (`NaN` hoáº·c vÃ´ cá»±c)

3.  **Data Leakage (RÃ² rá»‰ dá»¯ liá»‡u):**
    - Láº¥y thÃ´ng tin cá»§a táº­p Test Ä‘á»ƒ tÃ­nh toÃ¡n cho táº­p Train (vÃ­ dá»¥: tÃ­nh Mean/Std trÃªn toÃ n bá»™ dá»¯ liá»‡u trÆ°á»›c khi chia táº­p). Pháº£i chia táº­p trÆ°á»›c, rá»“i má»›i tÃ­nh Mean/Std trÃªn táº­p Train
